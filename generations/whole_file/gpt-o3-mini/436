from typing import Literal
import os
import datetime
import pandas as pd
import numpy as np
import yfinance as yf
from utils import *
import sqlite3
from tradecalendar import TradeCalendar


class Datafeed:
    def __init__(self, mode: Literal["backtest", "live"] = "backtest"):
        self.config = read_config_file("config/config.json")
        self.tickers = self.get_tickers_list()
        self.db_path = self.config.get("db_path", "stocks_data.db")
        self.mode = mode

        # Inicjalizuje TradeCalendar i daty
        self.trade_calendar = TradeCalendar(mode=self.mode, config=self.config)
        
        # Konwertuj daty konfiguracji na pd.Timestamp
        start_date_key = "download_start_date_live" if mode == "live" else "download_start_date"
        end_date_key = "download_end_date_live" if mode == "live" else "download_end_date"
        
        self.config_start = pd.to_datetime(self.config.get(start_date_key))
        self.config_end = pd.to_datetime(self.config.get(end_date_key))

        # Zainicjuj italy_holidays
        self.italy_holidays = self.trade_calendar.get_italian_holidays_with_custom_dates(
            self.config_start, self.config_end
        )

    def init_database(self):
        """Inizializza il database SQLite."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS daily_data (
                    date DATE,
                    ticker TEXT,
                    open REAL,
                    high REAL,
                    low REAL,
                    close REAL,
                    adj_close REAL,
                    volume INTEGER,
                    source TEXT,
                    manipulated TEXT,
                    insert_date DATE,
                    PRIMARY KEY (date, ticker)
                )
            """
            )

    def get_tickers_list(self) -> list:
        """Legge la lista dei ticker dal file di configurazione."""
        with open(self.config.get("tickers_list"), "r") as file:
            return [line.strip() for line in file if not line.startswith("#")]

    def identify_gaps(self, df: pd.DataFrame, ticker: str, business_days: pd.DatetimeIndex) -> pd.DataFrame:
        """
        Identifica i gap nella serie temporale considerando solo i giorni
        tra il primo e l'ultimo dato disponibile per il ticker.
        """
        # Upewnij się, że indeks DataFrame jest typu DatetimeIndex
        if isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index)

        # Uzyskaj pierwszą i ostatnią faktycznie dostępną datę dla symbolu giełdowego
        ticker_first_date = df.index.min()
        ticker_last_date = df.index.max()

        # Filtruj business_days, aby uwzględnić tylko te w zakresie dostępnych danych
        relevant_business_days = business_days[
            (business_days >= ticker_first_date) & (business_days <= ticker_last_date)
        ]

        # Identyfikuje brakujące dni w obrębie zakresu
        missing_days = set(relevant_business_days) - set(df.index)

        if missing_days:
            write_log(
                f"datafeed.py - {ticker}: Identificati {len(missing_days)} gap tra "
                f"{ticker_first_date.strftime('%Y-%m-%d')} e {ticker_last_date.strftime('%Y-%m-%d')}"
            )
            for day in sorted(missing_days):
                write_log(f"datafeed.py - {ticker}: Gap identificato in data {day.strftime('%Y-%m-%d')}")

            # Utwórz DataFrame dla brakujących dni
            gap_df = pd.DataFrame(
                index=sorted(missing_days), columns=df.columns, dtype=df.dtypes.to_dict()
            )
            df = pd.concat([df, gap_df]).sort_index()
        else:
            write_log(
                f"datafeed.py - {ticker}: Nessun gap identificato tra "
                f"{ticker_first_date.strftime('%Y-%m-%d')} e {ticker_last_date.strftime('%Y-%m-%d')}"
            )

        return df.reset_index()

    def fill_gaps(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:
        """
        Riempie i gap nei dati usando una strategia più robusta.
        """
        try:
            df_filled = df.copy()
            df_filled['date'] = pd.to_datetime(df_filled['date'])  # Upewnij się, że data jest typu datetime64
            df_filled = df_filled.set_index("date").resample('D').asfreq().reset_index()
            
            # Lista kolumn OHLCV
            price_cols = ["Open", "High", "Low", "Close", "Adj Close"]
            volume_cols = ["Volume"]

            # Statystyki do logowania
            fill_stats = {"ffill": 0, "bfill": 0, "interpolate": 0, "volume_fills": 0}

            # 1. Zarządzanie cenami (OHLC)
            for col in price_cols:
                # Identyfikacja brakujących wartości
                missing_mask = df_filled[col].isna()
                initial_missing = missing_mask.sum()

                if initial_missing > 0:
                    # Najpierw spróbuj z interpolacją liniową dla krótkich przerw (1-2 dni)
                    df_filled[col] = df_filled[col].interpolate(method="linear", limit=2)
                    interpolated = initial_missing - df_filled[col].isna().sum()
                    fill_stats["interpolate"] += interpolated

                    # Dla pozostałych, użyj ffill i bfill
                    before_ffill = df_filled[col].isna().sum()
                    df_filled[col] = df_filled[col].ffill()
                    after_ffill = df_filled[col].isna().sum()
                    fill_stats["ffill"] += before_ffill - after_ffill

                    # Zarządzaj pozostałymi wartościami NA za pomocą bfill
                    df_filled[col] = df_filled[col].bfill()
                    fill_stats["bfill"] += after_ffill

            # 2. Specjalne zarządzanie dla wolumenu
            for col in volume_cols:
                missing_mask = df_filled[col].isna()
                initial_missing = missing_mask.sum()

                if initial_missing > 0:
                    # Dla wolumenu oblicz średnią kroczącą z 5 poprzednich dni
                    rolling_mean = df_filled[col].rolling(window=5, min_periods=1).mean()
                    df_filled.loc[missing_mask, col] = rolling_mean[missing_mask]
                    fill_stats["volume_fills"] += initial_missing

            # Ostateczna weryfikacja
            remaining_na = df_filled.isna().sum()
            if remaining_na.any():
                write_log(
                    f"datafeed.py - ATTENZIONE: {ticker} ha ancora {remaining_na.sum()} valori NA dopo il filling"
                )
                for col in df_filled.columns:
                    if remaining_na[col] > 0:
                        write_log(
                            f"datafeed.py - {ticker}: Colonna {col} ha {remaining_na[col]} NA"
                        )

            # Log statystyk
            write_log(f"datafeed.py - Statistiche filling per {ticker}:")
            write_log(f"  - Interpolazioni: {fill_stats['interpolate']}")
            write_log(f"  - Forward fills: {fill_stats['ffill']}")
            write_log(f"  - Backward fills: {fill_stats['bfill']}")
            write_log(f"  - Volume fills: {fill_stats['volume_fills']}")

        except Exception as e:
            write_log(f"datafeed.py - Error in fill_gaps for {ticker}: {str(e)}")
            raise

        return df_filled

    def download_stocks_data(self, force_overwrite: bool = False) -> None:
        """Scarica i dati delle azioni."""
        write_log(f"datafeed.py - Avvio download dati nel range: {self.config_start} - {self.config_end}")

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            for ticker in self.tickers:
                try:
                    write_log(f"datafeed.py - Elaborazione {ticker}")

                    if force_overwrite:
                        self.download_yfinance(ticker, self.config_start, self.config_end, cursor)
                        continue

                    # Zmodyfikowane wykonanie zapytania i obsługa wyników
                    cursor.execute(
                        """
                        SELECT MIN(date) as min_date, MAX(date) as max_date 
                        FROM daily_data 
                        WHERE ticker = ?
                        """,
                        (ticker,),
                    )
                    existing_range = cursor.fetchone()

                    # Dodaj odpowiednią walidację wyniku zapytania
                    if existing_range is None or existing_range[0] is None:
                        write_log(f"datafeed.py - Nessun dato esistente per {ticker}, procedendo con il download completo")
                        self.download_yfinance(ticker, self.config_start, self.config_end, cursor)
                        continue

                    # Konwertuj daty z bazy danych na pd.Timestamp
                    existing_start = pd.to_datetime(existing_range[0])
                    existing_end = pd.to_datetime(existing_range[1])

                    if self.config_start >= existing_start and self.config_end <= existing_end:
                        write_log(f"datafeed.py - Dati già presenti per {ticker}")
                        continue

                    # Pobierz brakujące dane
                    if self.config_start < existing_start:
                        start_download = self.config_start
                        end_download = existing_start - pd.Timedelta(days=1)
                        self.download_yfinance(ticker, start_download, end_download, cursor)

                    if self.config_end > existing_end:
                        start_download = existing_end + pd.Timedelta(days=1)
                        end_download = self.config_end
                        self.download_yfinance(ticker, start_download, end_download, cursor)

                except Exception as e:
                    write_log(f"datafeed.py - Errore per {ticker}: {str(e)}")
                    continue  # Dodano continue, aby przejść do następnego tickera

            self.log_database_stats()

    def download_yfinance(self, ticker: str, start_date: pd.Timestamp, end_date: pd.Timestamp, cursor: sqlite3.Cursor) -> None:
        """
        Processa il download e salvataggio dei dati per un ticker specifico da yfinance
        """
        try:
            df = pd.DataFrame()  # Zainicjuj pusty DataFrame

            try:
                try:
                    df = yf.download(
                        ticker,
                        start=start_date,
                        end=end_date + pd.Timedelta(days=1),  # Dodaj jeden dzień, aby uwzględnić datę końcową
                        progress=False,
                    )
                except Exception as e:
                    write_log(f"datafeed.py - Error during download for {ticker}: {e}")
                    return
            except Exception as e:
                write_log(f"datafeed.py - Errore durante il download dei dati per {ticker}: {e}")
                return  # Pomiń dalsze przetwarzanie, jeśli pobieranie się nie powiedzie

            if df.empty:
                write_log(f"datafeed.py - No data downloaded for {ticker} in the specified period.")
                return

            # KRYTYCZNE: Natychmiast zresetuj indeks i dodaj kolumnę Ticker
            df = df.reset_index()
            df['Ticker'] = ticker
            df.rename(columns={'Date': 'date'}, inplace=True)  # Zmień nazwę Date na date
            df['date'] = pd.to_datetime(df['date'])

            # Wstawianie danych do bazy danych
            for _, row in df.iterrows():
                try:
                    date_value = row['date']
                    # Jeśli z jakiegoś powodu mamy jeszcze Series, bierzemy tylko wartość
                    if isinstance(date_value, pd.Series):
                        date_value = date_value.iloc[0]
                    
                    # Konwertuj na datetime, a następnie na stringa YYYY-MM-DD
                    date_str = pd.to_datetime(date_value).strftime('%Y-%m-%d')
                    
                    cursor.execute(
                        """
                        INSERT OR REPLACE INTO daily_data 
                        (date, ticker, open, high, low, close, adj_close, volume, source)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (
                            date_str,           # Użyj oczyszczonej wartości daty
                            ticker,             # Użyj bezpośrednio przekazanego jako parametr tickera
                            float(row['Open']),
                            float(row['High']),
                            float(row['Low']),
                            float(row['Close']),
                            float(row['Adj Close']),
                            int(row['Volume']),
                            "yfinance"
                        )
                    )
                except Exception as e:
                    write_log(f"datafeed.py - Errore nell'inserimento della riga per {ticker} "
                            f"data {date_value}: {str(e)}")
                    continue

            cursor.connection.commit()
            write_log(f"datafeed.py - Download completato per {ticker}")

        except Exception as e:
            write_log(f"datafeed.py - Errore critico durante l'elaborazione di {ticker}: {str(e)}")
            raise
  
    def verify_data_completeness(self) -> None:
        """Verifica la completezza dei dati per ogni ticker."""
        write_log(f"datafeed.py - Start of the verification of data completeness")
        with sqlite3.connect(self.db_path) as conn:
            df_dates = pd.read_sql_query("SELECT DISTINCT date FROM daily_data", conn)
            # Użyj bardziej elastycznego parsowania daty
            min_date = pd.to_datetime(df_dates["date"])
            max_date = pd.to_datetime(df_dates["date"]).max()

            for ticker in self.tickers:
                missing_days = self._find_missing_days(ticker)
                if missing_days is None:
                    write_log(f"datafeed.py - Ticker {ticker} non presente nel dataset")
                    continue

                write_log(f"datafeed.py - Analisi completezza dati per {ticker}:")

                if missing_days:
                    write_log(f"datafeed.py - Giorni lavorativi mancanti per {ticker}:")
                    for day in sorted(missing_days):
                        write_log(f"datafeed.py - {ticker}: Giorno mancante {day}")
                else:
                    write_log(f"datafeed.py - Tutti i giorni lavorativi presenti per {ticker}")

    def log_database_stats(self):
        """Logga le statistiche del database."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT MIN(date), MAX(date) FROM daily_data")
            min_date, max_date = cursor.fetchone()
            write_log(f"datafeed.py - Range date finale nel database: {min_date} - {max_date}")

            for ticker in self.tickers:
                cursor.execute(
                    "SELECT MIN(date), MAX(date) FROM daily_data WHERE ticker = ?", (ticker,)
                )
                result = cursor.fetchone()
                if result and result[0]:
                    write_log(
                        f"datafeed.py - {ticker} - Date range finale: {result[0]} - {result[1]}"
                    )

    def fill_zero_values(self, df: pd.DataFrame, lookback_period: int = 5) -> pd.DataFrame:
        """
        Fills zero values in OHLCV columns using specific strategies for each data type.

        Args:
            df: DataFrame with the data to fill.
            lookback_period: Lookback period for calculating averages.

        Returns:
            DataFrame with filled zero values.
        """
        df_filled = df.copy()
        df_filled = df_filled.reset_index()
        price_cols = ["Open", "High", "Low", "Close", "Adj Close"]
        volume_cols = ["Volume"]

        fill_stats = {}

        for ticker in df_filled['Ticker'].unique():
            ticker_stats = {
                "price_fills": {col: 0 for col in price_cols},
                "volume_fills": 0,
                "mean_fills": 0,
                "nearest_fills": 0,
            }

            ticker_data = df_filled[df_filled['Ticker'] == ticker].copy()
            ticker_data = ticker_data.set_index('date')

            # Konwertuj indeks daty na DatetimeIndex, jeśli jeszcze nim nie jest
            if not isinstance(ticker_data.index, pd.DatetimeIndex):
                ticker_data.index = pd.to_datetime(ticker_data.index)

            if ticker_data.index.duplicated().any():
                write_log(f"datafeed.py - Duplicate dates found for {ticker}, keeping first occurrence.")
                ticker_data = ticker_data[~ticker_data.index.duplicated(keep='first')]

            try:
                # 1. Zarządzanie cenami
                for col in price_cols:
                    zero_mask = ticker_data[col] == 0
                    zero_dates = ticker_data[zero_mask].index

                    if len(zero_dates) > 0:
                        for date in zero_dates:
                            prev_data = ticker_data.loc[:date][col]
                            prev_data = prev_data[prev_data != 0][-lookback_period:]

                            next_data = ticker_data.loc[date:][col]
                            next_data = next_data[next_data != 0][:lookback_period]

                            if len(prev_data) > 0 and len(next_data) > 0:
                                prev_val = prev_data.iloc[-1]
                                next_val = next_data.iloc[0]
                                weighted_val = (prev_val + next_val) / 2
                                ticker_data.loc[date, col] = weighted_val
                                ticker_stats["mean_fills"] += 1
                            elif len(prev_data) > 0:
                                ticker_data.loc[date, col] = prev_data.iloc[-1]
                                ticker_stats["nearest_fills"] += 1
                            elif len(next_data) > 0:
                                ticker_data.loc[date, col] = next_data.iloc[0]
                                ticker_stats["nearest_fills"] += 1

                            ticker_stats["price_fills"][col] += 1

                # 2. Zarządzanie Wolumenem
                for col in volume_cols:
                    zero_mask = ticker_data[col] == 0
                    zero_dates = ticker_data[zero_mask].index

                    if len(zero_dates) > 0:
                        for date in zero_dates:
                            surrounding_data = ticker_data[
                                (ticker_data.index >= date - pd.Timedelta(days=lookback_period)) &
                                (ticker_data.index <= date + pd.Timedelta(days=lookback_period))
                            ][col]
                            non_zero_vol = surrounding_data[surrounding_data != 0]

                            if len(non_zero_vol) > 0:
                                ticker_data.loc[date, col] = int(non_zero_vol.mean())
                                ticker_stats["volume_fills"] += 1

                # Zaktualizuj df_filled za pomocą zmodyfikowanego ticker_data
                for col in price_cols + volume_cols:
                    if col in price_cols:
                        df_filled.loc[df_filled['Ticker'] == ticker, col] = ticker_data[col].astype(float)
                    elif col in volume_cols:
                        df_filled.loc[df_filled['Ticker'] == ticker, col] = ticker_data[col].astype(int)
                fill_stats[ticker] = ticker_stats

            except Exception as e:
                write_log(f"datafeed.py - Error during zero value filling for {ticker}: {str(e)}")

        df_filled = df_filled.reset_index()  # Upewnij się, że zawsze zwracamy datę jako kolumnę, a nie indeks
        self._write_detailed_fill_stats(fill_stats)
        self._verify_remaining_zeros(df_filled, price_cols + volume_cols)
        return df_filled

    def _write_detailed_fill_stats(self, fill_stats: dict):
        """Scrive statistiche dettagliate delle operazioni di filling."""
        log_file_path = os.path.join("log", "fill_zero_detailed.txt")
        with open(log_file_path, "w") as f:
            f.write("Report dettagliato operazioni di fill zero:\n")
            f.write("=" * 80 + "\n\n")

            for ticker, stats in fill_stats.items():
                f.write(f"Ticker: {ticker}\n")
                f.write("-" * 40 + "\n")
                f.write("Prezzi:\n")
                for col, fills in stats["price_fills"].items():
                    f.write(f"  - {col}: {fills} fills\n")
                f.write(f"Volume: {stats['volume_fills']} fills\n")
                f.write(f"Fills con media: {stats['mean_fills']}\n")
                f.write(f"Fills con valore più vicino: {stats['nearest_fills']}\n\n")

    def _verify_remaining_zeros(self, df: pd.DataFrame, columns: list):
        """Verifica e logga eventuali valori zero rimanenti."""
        zero_counts = (df[columns] == 0).sum()
        if zero_counts.sum() > 0:
            write_log("datafeed.py - ATTENZIONE: Valori zero rimanenti dopo il filling:")
            for col in columns:
                if zero_counts[col] > 0:
                    write_log(f"  - {col}: {zero_counts[col]} valori zero")

                    # Szczegółowy log pozostałych wartości zero
                    zero_mask = df[col] == 0
                    zero_records = df[zero_mask]
                    for idx, row in zero_records.iterrows():
                        write_log(f"    * Ticker: {row['Ticker']} in data {idx}")

    def load_data_from_db(self) -> pd.DataFrame:
        """Carica i dati dal database."""
        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query(
                """
                SELECT date, ticker, open, high, low, close, adj_close, volume
                FROM daily_data
                """,
                conn,
            )

            df["date"] = pd.to_datetime(df["date"], format="%Y-%m-%d")

            # Zmień nazwę kolumny 'ticker' na 'Ticker
            df = df.rename(columns={"ticker": "Ticker"})

            df = df.set_index(["Ticker", "date"])
            df.columns = ["Open", "High", "Low", "Close", "Adj Close", "Volume"]

            return df

    def save_data_to_db(self, df: pd.DataFrame) -> None:
        """
        Salva i dati elaborati nel database usando INSERT OR REPLACE.
        Aggiunge la colonna "insert_date" con la data e ora di sistema al momento dell'inserimento.
        """
        conn = sqlite3.connect(self.db_path)
        try:
            df_to_save = df.copy().reset_index()  # Zawsze resetuj indeks
            # Przekształć datę na datetime, jeśli jeszcze nie jest, i poprawnie obsłuż nieprawidłowe daty
            if not pd.api.types.is_datetime64_any_dtype(df_to_save['date']):
                df_to_save['date'] = pd.to_datetime(df_to_save['date'], errors='raise')  # teraz zgłoś błąd
            df_to_save['date'] = df_to_save['date'].dt.strftime('%Y-%m-%d')
            
            # Zmień nazwy kolumn, aby pasowały do schematu bazy danych
            column_mapping = {
                'Ticker': 'ticker',
                'Open': 'open',
                'High': 'high',
                'Low': 'low',
                'Close': 'close',
                'Adj Close': 'adj_close',
                'Volume': 'volume'
            }
            
            # Zmień nazwę tylko tych kolumn, które istnieją
            for old_col, new_col in column_mapping.items():
                if old_col in df_to_save.columns:
                    df_to_save = df_to_save.rename(columns={old_col: new_col})
            
            # Dodaj kolumny source i insert_date, jeśli nie są obecne
            if 'source' not in df_to_save.columns:
                df_to_save['source'] = 'Yahoo Finance'
            if 'insert_date' not in df_to_save.columns:
                df_to_save['insert_date'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
            # Upewnij się, że wszystkie wymagane kolumny są obecne
            required_columns = ['date', 'ticker', 'open', 'high', 'low', 'close', 
                              'adj_close', 'volume', 'source', 'insert_date']
            missing_columns = set(required_columns) - set(df_to_save.columns)
            if missing_columns:
                write_log(f"datafeed.py - Missing required columns: {missing_columns}")
                raise ValueError(f"Missing required columns: {missing_columns}")
            # Rejestrowanie debugowania
            write_log(f"datafeed.py - Final columns before save: {df_to_save.columns.tolist()}")
            write_log(f"datafeed.py - Number of rows to save: {len(df_to_save)}")
            
            # Wstaw dane za pomocą executemany dla lepszej wydajności
            cursor = conn.cursor()
            data_to_insert = df_to_save[required_columns].values.tolist()
            
            cursor.executemany(
                """
                INSERT OR REPLACE INTO daily_data 
                (date, ticker, open, high, low, close, adj_close, volume, source, insert_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                data_to_insert
            )
            
            conn.commit()
            write_log(f"datafeed.py - Successfully saved {len(df_to_save)} records to database")
            
        except Exception as e:
            conn.rollback()
            write_log(f"datafeed.py - Error saving data to database: {str(e)}")
            raise
        finally:
            conn.close()

    def fill_gaps_for_all_tickers(self):
        """Riempie i gap per tutti i ticker nel database."""
        with sqlite3.connect(self.db_path) as conn:
            for ticker in self.tickers:
                try:
                    missing_days = self._find_missing_days(ticker)
                    if missing_days is None:
                        write_log(f"datafeed.py - Nessun dato esistente per {ticker}")
                        continue

                    write_log(f"datafeed.py - Inizio riempimento gap per {ticker}")
                    write_log(f"datafeed.py - {ticker}: Esecuzione query SQL per load dati in dataframe")
                    df = pd.read_sql_query(
                        """
                        SELECT date, open, high, low, close, adj_close, volume
                        FROM daily_data
                        WHERE ticker = ?
                        ORDER BY date
                        """,
                        conn,
                        params=(ticker,),
                    )
                    write_log(f"datafeed.py - {ticker}: Query SQL completata")
                    if df.empty:
                        write_log(f"datafeed.py - Nessun dato esistente per {ticker}")
                        continue

                    write_log(f"datafeed.py - {ticker}: Conversione colonna 'date' in datetime")
                    df['date'] = pd.to_datetime(df['date'])
                    write_log(f"datafeed.py - {ticker}: Conversione completata")

                    existing_start = df['date'].min()
                    existing_end = df['date'].max()
                    write_log(f"datafeed.py - {ticker}: Data inizio esistente: {existing_start}")
                    write_log(f"datafeed.py - {ticker}: Data fine esistente: {existing_end}")

                    write_log(f"datafeed.py - {ticker}: Calcolo business days")
                    business_days = self.trade_calendar.get_business_days(
                        existing_start, existing_end, self.italy_holidays
                    )
                    write_log(f"datafeed.py - {ticker}: Calcolo business days completato")

                    write_log(f"datafeed.py - {ticker}: Impostazione indice del DataFrame su date")
                    df = df.set_index('date')
                    write_log(f"datafeed.py - {ticker}: Indice impostato")

                    write_log(f"datafeed.py - {ticker}: Identificazione gap")
                    df_filled = self.identify_gaps(df, ticker, business_days)
                    write_log(f"datafeed.py - {ticker}: Identificazione gap completata")

                    write_log(f"datafeed.py - {ticker}: Riempimento gap")
                    df_filled = self.fill_gaps(df_filled, ticker)
                    write_log(f"datafeed.py - {ticker}: Riempimento gap completato")

                    write_log(f"datafeed.py - {ticker}: Salvataggio dati")
                    self.save_data_to_db(df_filled)
                    write_log(f"datafeed.py - {ticker}: Salvataggio dati completato")
                except Exception as e:
                    write_log(f"datafeed.py - Errore durante il riempimento dei gap per {ticker}: {str(e)}")
                    continue

    def _find_missing_days(self, ticker):
        with sqlite3.connect(self.db_path) as conn:
            df_ticker = pd.read_sql_query(
                "SELECT date FROM daily_data WHERE ticker = ?", conn, params=(ticker,)
            )
            if df_ticker.empty:
                return None  # Lub zgłoś wyjątek, w zależności od pożądanego zachowania

            df_ticker["date"] = pd.to_datetime(df_ticker["date"])
            ticker_start = df_ticker["date"].min()
            ticker_end = df_ticker["date"].max()

            business_days = self.trade_calendar.get_business_days(
                ticker_start, ticker_end, self.italy_holidays
            )
            missing_days = set(business_days) - set(df_ticker["date"])
            return missing_days


if __name__ == "__main__":
    datafeed = Datafeed()
    # datafeed.pobierz_dane_akcji()
    datafeed.verify_data_completeness()
    datafeed.fill_gaps_for_all_tickers()
    # df = datafeed.wczytaj_dane_z_bazy()
    # df_filled = datafeed.wypełnij_wartości_zero(df)
    # datafeed.zapisz_dane_do_bazy(df_filled)