import time
import torch
import numpy as np
from torch.utils.data import DataLoader
from transformers import TrainerCallback, default_data_collator

# 定义 FactualAccuracyCallbackBETTER 类（如所提供）
class FactualAccuracyCallbackBETTER(TrainerCallback):
    """“一个回调函数，用于在训练期间评估和记录模型的事实准确性。”"""

    def __init__(
        self, model, tokenizer, dataset, batch_size, verbose=False, output_format=False
    ):
        super().__init__()
        self.model = model
        self.tokenizer = tokenizer
        self.n_samp = len(dataset)
        self.verbose = verbose
        self.output_format = output_format
        tokenized_questions = dataset.map(
                lambda examples: tokenizer(examples["question"], padding="max_length", truncation=True, max_length=512,),
                batched=True,
            )
        self.batched_tokenized_questions = DataLoader(tokenized_questions, batch_size=batch_size, shuffle=False, collate_fn=default_data_collator)
        self.batched_expected_answers = DataLoader(dataset['answer'], batch_size=batch_size, shuffle=False)


    def on_log(self, args, state, control, model=None, **kwargs):
        """在记录最后的日志后调用。"""
        if model is not None:
            self.model = model
        elif self.model is None:
            return

        if not state.is_local_process_zero:
            return

        start_time = time.time()
        try:
            with torch.no_grad():
                results = factual_score_dataloader(
                            model=model,
                            tokenizer=self.tokenizer,
                            batched_tokenized_questions=self.batched_tokenized_questions,
                            expected_answers=self.batched_expected_answers,
                            output_format=self.output_format,
                        )
                if self.output_format:
                    factual_accuracy_avg, format_hard_avg, format_soft_avg = results
                else:
                    factual_accuracy_avg = results

                if len(state.log_history) > 0:
                    state.log_history[-1]["factual_accuracy"] = factual_accuracy_avg
                    if self.output_format:
                        state.log_history[-1]["format_hard"] = format_hard_avg
                        state.log_history[-1]["format_soft"] = format_soft_avg
        except Exception as e:
            print(f"Error during factual accuracy evaluation: {e}")
        finally:
            time_taken = time.time() - start_time
            if self.verbose:
                print(f"[TIME] {time_taken:.2f} seconds: Model evaluated on FactualAccuracy.")

def check_answer_factual(*args):
    pass

def check_answer_format(*args):
    pass

def factual_score_dataloader(
    model,
    tokenizer,
    batched_tokenized_questions,
    expected_answers,
    max_new_tokens=32,
    output_format=False,
    random_state=42,
    device=None,
    verbose=False,
):
    """
    Evaluate the factual accuracy of answers from a language model.

    Args:
        model: The language model.
        tokenizer: The tokenizer.
        tokenized_eval_dataset: The tokenized evaluation dataset.
        max_new_tokens: Maximum number of new tokens to generate.
        output_format: Whether to check output format.
        random_state: Random seed for sampling.
        device: Device to run on (defaults to CUDA if available, else CPU).

    Returns:
        fact_results: List of factual accuracy results (boolean) or final rolling average.
        format_hard_results (optional): Rolling average of hard format check results.
        format_soft_results (optional): Rolling average of soft format check results.
    """

    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    fact_results = []  # Will store rolling averages for factual accuracy.
    # 修改开始：不再存储格式检查结果的列表，而是计算滚动平均值
    fact_mean = 0
    if output_format:
        format_hard_mean = 0
        format_soft_mean = 0
    count = 0
    # 修改结束

    for batch, expected_answers in zip(batched_tokenized_questions, expected_answers):
        batch = {k: v.to(device) for k, v in batch.items() if k in ["input_ids", "attention_mask"]}

        with torch.no_grad():
            outputs = model.generate(
                **batch,
                max_new_tokens=max_new_tokens,
                pad_token_id=tokenizer.pad_token_id
            )
            detokenized_inputs = tokenizer.batch_decode(batch["input_ids"], skip_special_tokens=True)
            output_strings = tokenizer.batch_decode(outputs[:, batch["input_ids"].shape[-1]:], skip_special_tokens=True)
            
            # 使用列表推导式来提高性能
            new_results = [check_answer_factual(output_str, expected_answer) for output_str, expected_answer in zip(output_strings, expected_answers)]
            # 计算事实准确性的滚动平均值
            fact_mean = (fact_mean * count + sum(new_results)) / (count + len(new_results))
            prev_count = count
            count += len(new_results)
            fact_results.append(fact_mean)
            if output_format:
                # 计算格式检测的滚动平均值，而非将结果存入列表中
                batch_hard = sum([check_answer_format(output_str, hard=True) for output_str in output_strings])
                batch_soft = sum([check_answer_format(output_str, hard=False) for output_str in output_strings])
                # 使用之前的count值来更新滚动平均值
                format_hard_mean = (format_hard_mean * prev_count + batch_hard) / count
                format_soft_mean = (format_soft_mean * prev_count + batch_soft) / count
            
    return (fact_mean, format_hard_mean, format_soft_mean) if output_format else fact_mean