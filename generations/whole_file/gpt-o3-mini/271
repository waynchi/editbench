import pytest
import yaml
from collections import Counter
import numpy as np
from scipy import stats
from fastapi.testclient import TestClient
from app import fastapp  # Импортировать существующий экземпляр приложения FastAPI
from src.utils import get_settings


@pytest.fixture(scope="session")
def fast_app():
    """Получить экземпляр FastAPIApp из существующего приложения"""
    return fastapp


@pytest.fixture(scope="session")
def n_trials():
    """Количество испытаний для тестирования распределения"""
    return 300000


def get_ground_truth_probabilities():
    """Извлечь истинные вероятности из YAML файла конфигурации. Возвращает словарь с именами моделей и их нормализованными вероятностями."""
    # Прочитать YAML файл
    config = get_settings()

    # Извлечь веса для активных моделей (не закомментированных)
    model_weights = {
        model_name: model_info["weight"]
        for model_name, model_info in config["models"].items()
    }

    # Вычислить общий вес для нормализации
    total_weight = sum(model_weights.values())

    # Вычислить нормализованные вероятности
    probabilities = {
        model_name: weight / total_weight
        for model_name, weight in model_weights.items()
    }

    return probabilities


def calculate_expected_paired_probabilities(ground_truth_probs):
    """Вычислить ожидаемые вероятности при выборке пар без замены.

Для каждой модели M, её общая вероятность:
P(M) = P(M выбрана первой) + P(M выбрана второй)
= P(M первая) + сумма[P(другая первая) * P(M вторая | другая первая)]"""
    models = list(ground_truth_probs.keys())
    n_models = len(models)
    adjusted_probs = {}

    for model in models:
        prob = 0
        # Вероятность быть выбранным первым
        prob_first = ground_truth_probs[model]

        # Вероятность быть выбранным вторым
        for other_model in models:
            if other_model != model:
                # Если other_model выбран первым (prob_first_other),
                # тогда вероятность выбора модели второй равна её весу, делённому на
                # сумма всех весов, кроме веса other_model
                prob_first_other = ground_truth_probs[other_model]
                remaining_weight = sum(
                    ground_truth_probs[m] for m in models if m != other_model
                )
                prob_second_given_first = ground_truth_probs[model] / remaining_weight
                prob += prob_first_other * prob_second_given_first

        # Общая вероятность - это сумма вероятностей быть выбранным первым или вторым
        total_prob = prob_first + prob
        adjusted_probs[model] = total_prob

    # Нормализовать вероятности
    total = sum(adjusted_probs.values())
    return {model: prob / total for model, prob in adjusted_probs.items()}


def test_model_distribution(fast_app, n_trials):
    """Проверить, соответствует ли распределение индивидуальных выборов моделей ожидаемым вероятностям"""
    # Получить истинные вероятности из конфигурации
    ground_truth_probs = get_ground_truth_probabilities()

    # Рассчитать скорректированные вероятности для парного отбора
    expected_probs = calculate_expected_paired_probabilities(ground_truth_probs)

    # Собрать образцы - подсчитать каждую модель отдельно
    selected_models = []
    for _ in range(n_trials):
        models, _, _ = fast_app.select_models(tags=[])
        selected_models.extend(models)

    # Подсчитать количество вхождений каждой модели
    model_counts = Counter(selected_models)

    # Рассчитать общее количество выборов (2 модели на испытание)
    total_selections = n_trials * 2

    # Вывести анализ
    print("\nModel Distribution Analysis:")
    print("\nProbability Comparison:")
    print(
        f"{'Model':<30} {'Original':<12} {'Adjusted':<12} {'Observed':<12} {'Diff %':<10}"
    )
    print("-" * 75)

    # Подготовка массивов для критерия хи-квадрат
    observed_freqs = []
    expected_freqs = []

    for model in sorted(ground_truth_probs.keys()):
        original_prob = ground_truth_probs[model]
        expected_prob = expected_probs[model]
        observed_count = model_counts[model]
        observed_prob = observed_count / total_selections
        diff_percent = ((observed_prob - expected_prob) / expected_prob) * 100

        print(
            f"{model:<30} {original_prob:>11.4f} {expected_prob:>11.4f} "
            f"{observed_prob:>11.4f} {diff_percent:>+9.1f}%"
        )

        # Добавить в массивы для хи-квадрат теста
        expected_freqs.append(expected_prob * total_selections)
        observed_freqs.append(observed_count)

    # Выполнить тест хи-квадрат
    chi2, p_value = stats.chisquare(observed_freqs, expected_freqs)

    print("\nStatistical Analysis:")
    print(f"Total selections: {total_selections}")
    print(f"Chi-square statistic: {chi2:.4f}")
    print(f"P-value: {p_value:.4f}")

    # Убедитесь, что p-значение выше порога
    assert (
        p_value > 0.05
    ), f"Distribution of selected models differs significantly from expected (p={p_value:.4f})"


def test_tag_filtering(fast_app):
    """Проверка, учитывает ли выбор модели фильтрацию по тегам"""
    # Тест с конкретным тегом
    test_tag = list(fast_app.tag_to_models.keys())[0]  # Получить первый доступный тег
    tagged_models = fast_app.tag_to_models[test_tag]

    # Выбрать несколько раз с тегом
    for _ in range(100):
        models, client1, client2 = fast_app.select_models(tags=[test_tag])
        # Проверить, имеют ли выбранные модели требуемый тег
        assert all(
            model in tagged_models for model in models
        ), f"Selected models {models} don't all have tag {test_tag}"


def test_different_models(fast_app):
    """Проверка, возвращает ли select_models всегда две разные модели"""
    for _ in range(100):
        models, _, _ = fast_app.select_models(tags=[])
        assert len(set(models)) == 2, f"Selected models {models} are not unique"


def test_empty_tags_uses_all_models(fast_app):
    """Проверка, использует ли пустой список тегов все доступные модели"""
    all_models = set()
    n_trials = 1000

    # Запустить несколько испытаний, чтобы убедиться, что мы видим все возможные модели
    for _ in range(n_trials):
        models, _, _ = fast_app.select_models(tags=[])
        all_models.update(models)

    # Проверить, видели ли мы все доступные модели
    assert all_models == set(
        fast_app.models
    ), f"Not all models were selected. Missing: {set(fast_app.models) - all_models}"


def test_model_client_mapping(fast_app):
    """Проверка, соответствуют ли возвращенные клиенты выбранным моделям"""
    for _ in range(100):
        models, client1, client2 = fast_app.select_models(tags=[])

        # Проверить, соответствуют ли клиенты своим моделям
        assert (
            models[0] in client1.models
        ), f"Client 1 doesn't support model {models[0]}"
        assert (
            models[1] in client2.models
        ), f"Client 2 doesn't support model {models[1]}"


def test_model_position_distribution(fast_app, n_trials):
    """Проверить, появляется ли каждая модель примерно одинаково часто на первой и второй позиции"""
    # Отслеживать позиции для каждой модели
    position_counts = {}  # {model: [количество_первых_позиций, количество_вторых_позиций]}

    # Собрать образцы
    for _ in range(n_trials):
        models, _, _ = fast_app.select_models(tags=[])

        # Инициализировать счетчики для новых моделей
        for model in models:
            if model not in position_counts:
                position_counts[model] = [0, 0]

        # Считать позиции (индекс 0 для первой позиции, 1 для второй позиции)
        position_counts[models[0]][0] += 1
        position_counts[models[1]][1] += 1

    # Вывод и анализ результатов
    print("\nPosition Distribution Analysis:")
    print(f"{'Model':<30} {'First Pos %':<12} {'Second Pos %':<12} {'Diff %':<10}")
    print("-" * 65)

    # Для каждой модели проверить, что процент выборов для первой и второй позиции находится в пределах 2% от 50%
    for model in sorted(position_counts.keys()):
        first_count = position_counts[model][0]
        second_count = position_counts[model][1]
        total_count = first_count + second_count

        if total_count == 0:
            continue

        first_percent = (first_count / total_count) * 100
        second_percent = (second_count / total_count) * 100
        diff_percent = first_percent - second_percent

        print(
            f"{model:<30} {first_percent:>11.1f} {second_percent:>11.1f} "
            f"{diff_percent:>+9.1f}"
        )

        # Проверка, что процент первого и второго выбора модели находится в пределах 2% от 50%
        assert abs(first_percent - 50) <= 2, (
            f"Model {model} shows position bias: first position percentage {first_percent:.1f}% is not within 2% of 50%"
        )
        assert abs(second_percent - 50) <= 2, (
            f"Model {model} shows position bias: second position percentage {second_percent:.1f}% is not within 2% of 50%"
        )