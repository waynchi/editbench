from dataclasses import dataclass
import numpy as np
norm = np.random.normal

@dataclass
class NeuralNetwork:
  # węzły wejściowe (liczba neuronów na wejściu, czyli liczba wejść)
  inp: int  
  # ukryte węzły (liczba neuronów w warstwie ukrytej, czyli ile liczb przetwarzamy w warstwie ukrytej)
  hid: int  
  # output nodes (liczba neuronów wyjściowych, czyli liczba w wyjściu)
  out: int  
  # współczynnik uczenia się (współczynnik wygładzania alfa)
  lr: float  
  # funkcja aktywacji (zależność wyjścia neuronu od wejścia do neuronu)
  act: callable  
  # epoki (liczba epok uczenia)
  epo: int  

  def __post_init__(self):
    # Poprawienie błędu: używamy odwrotności pierwiastka z liczby wejść/ukrytych, zgodnie z zasadami inicjalizacji wag.
    self.wih = norm(0., 1. / np.sqrt(self.inp), (self.hid, self.inp))
    self.who = norm(0., 1. / np.sqrt(self.hid), (self.out, self.hid))

  def train(self, x, y):
    x = np.array(x, ndmin=2).T
    y = np.array(y, ndmin=2).T

    ho = self.act(self.wih @ x)  # ukryte wyjścia
    fo = self.act(self.who @ ho) # ostateczne wyjścia
    oe = y - fo                # błędy wyjściowe
    he = self.who.T @ oe         # ukryte błędy
    self.who += self.lr * (oe * fo * (1. - fo)) @ ho.T
    self.wih += self.lr * (he * ho * (1. - ho)) @ x.T

  def query(self, x):
    x = np.array(x, ndmin=2).T
    return self.act(self.who @ self.act(self.wih @ x))

  def fit(self, X, y):
    for e in range(self.epo):
      for i in range(len(y)):
        self.train(X[i], y[i])

  def predict(self, X):
    return np.array([np.argmax(self.query(x)) for x in X])

  def score(self, X, y):
    y = np.array([np.argmax(i) for i in y])
    return (self.predict(X) == y).mean()