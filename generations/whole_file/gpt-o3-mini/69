import numpy as np
from matplotlib import pyplot as plt
from scipy.stats import lognorm
from scipy.optimize import minimize
from scipy.integrate import quad
import pandas as pd
from tqdm import tqdm
from typing import Dict, List, Tuple
import json
import pandas as pd


class ModelRouter:
    def __init__(
        self,
        models: List[str],
        lambda_latency: float = 1.0,
        lambda_rarity: float = 1.0,
        lambda_ambiguity: float = 1.0,
    ):
        self.models = models
        self.n_models = len(models)
        self.model_to_idx = {model: idx for idx, model in enumerate(models)}
        self.lambda_latency = lambda_latency
        self.lambda_rarity = lambda_rarity
        self.lambda_ambiguity = lambda_ambiguity

        # Initialize parameters
        self.n_pairs = (self.n_models * (self.n_models - 1)) // 2
        self.theta = np.zeros(self.n_pairs)

        # Cache for battle statistics
        self.battle_counts = None
        self.battle_preferences = None

        # Cache for latency parameters
        self.latency_params = None

    def _softmax_function(self, theta: np.ndarray, temp: float = 1.0) -> np.ndarray:
        """Convert parameters to probabilities using softmax with temperature."""
        exp_theta = np.exp(theta / temp)
        return exp_theta / np.sum(exp_theta)

    def _pair_to_index(self, i: int, j: int) -> int:
        """Convert model pair indices to flat index."""
        if i > j:
            i, j = j, i
        return i * (self.n_models - 1) - (i * (i - 1)) // 2 + (j - i - 1)

    def _index_to_pair(self, idx: int) -> Tuple[int, int]:
        """Convert flat index to model pair indices."""
        i = 0
        while idx >= self.n_models - i - 1:
            idx -= self.n_models - i - 1
            i += 1
        j = i + idx + 1
        return i, j

    def fit_latency_parameters(self, completions_df: pd.DataFrame):
        """Fit log-normal parameters for each model's latency distribution."""
        self.latency_params = {}

        for model in self.models:
            model_latencies = completions_df[completions_df["model"] == model][
                "latency"
            ]
            model_latencies = model_latencies[np.isfinite(model_latencies)]

            if len(model_latencies) > 0:
                # Fit log-normal distribution
                shape, loc, scale = lognorm.fit(model_latencies, floc=0)
                # Convert to mu and sigma parameters
                mu = np.log(scale)
                sigma = shape
                self.latency_params[model] = (mu, sigma)
            else:
                print(f"Warning: No latency data for model {model}")
                self.latency_params[model] = (0, 1)  # Default parameters

        print(self.latency_params)

    def compute_battle_statistics(self, outcomes_df: pd.DataFrame):
        """Compute battle counts and preferences from outcomes data."""
        battle_counts = np.zeros((self.n_models, self.n_models))
        battle_preferences = np.zeros((self.n_models, self.n_models))

        for _, row in outcomes_df.iterrows():
            items = (
                json.loads(row["completionItems"])
                if isinstance(row["completionItems"], str)
                else row["completionItems"]
            )

            if len(items) < 2:
                continue

            # Consider only the first two models in each battle
            model1, model2 = items[0]["model"], items[1]["model"]
            if model1 not in self.model_to_idx or model2 not in self.model_to_idx:
                continue

            i, j = self.model_to_idx[model1], self.model_to_idx[model2]
            battle_counts[i, j] += 1
            battle_counts[j, i] += 1

            # Determine preference using acceptedIndex
            if row.get("acceptedIndex") == 0:
                battle_preferences[i, j] += 1
                battle_preferences[j, i] -= 1
            elif row.get("acceptedIndex") == 1:
                battle_preferences[i, j] -= 1
                battle_preferences[j, i] += 1

        self.battle_counts = battle_counts
        self.battle_preferences = battle_preferences

    def compute_latency(self):
        """Compute expected maximum latency objective using exact PDF/CDF calculation."""

        def max_latency_integrand(
            l: float, mu_i: float, sigma_i: float, mu_j: float, sigma_j: float
        ) -> float:
            """
            Compute the density function for max latency:
            f_max(l) = f(l;mu_i,sigma_i)F(l;mu_j,sigma_j) + F(l;mu_i,sigma_i)f(l;mu_j,sigma_j)
            """
            # PDF for model i
            f_i = lognorm.pdf(l, sigma_i, scale=np.exp(mu_i))
            # CDF for model j
            F_j = lognorm.cdf(l, sigma_j, scale=np.exp(mu_j))
            # PDF for model j
            f_j = lognorm.pdf(l, sigma_j, scale=np.exp(mu_j))
            # CDF for model i
            F_i = lognorm.cdf(l, sigma_i, scale=np.exp(mu_i))

            max_latency = l * (f_i * F_j + F_i * f_j)
            return max_latency

        total_latency = 0
        self.latencies = []

        for idx in range(self.n_pairs):
            i, j = self._index_to_pair(idx)
            mu_i, sigma_i = self.latency_params[self.models[i]]
            mu_j, sigma_j = self.latency_params[self.models[j]]

            # Integrate the max latency density function from 0 to infinity
            expected_max, _ = quad(
                max_latency_integrand, 0, np.inf, args=(mu_i, sigma_i, mu_j, sigma_j)
            )

            self.latencies.append(expected_max)

        self.latencies = np.array(self.latencies)

        self.normalized_latencies = (self.latencies - min(self.latencies)) / (
            max(self.latencies) - min(self.latencies)
        )

    def compute_latency_objective(self, probs: np.ndarray) -> float:

        total_normalized_latency = sum(
            [probs[idx] * self.normalized_latencies[idx] for idx in range(self.n_pairs)]
        )

        return total_normalized_latency

    def compute_rarity_objective(self, probs: np.ndarray) -> float:
        """Compute rarity objective."""
        epsilon = 1.0  # Smoothing factor
        rarity_scores = []
        total_rarity = 0
        for idx in range(self.n_pairs):
            i, j = self._index_to_pair(idx)
            count = self.battle_counts[i, j]
            rarity_score = 1.0 / (count + epsilon)
            rarity_scores.append(rarity_score)
            total_rarity -= probs[idx] * rarity_score

        return total_rarity

    def compute_ambiguity_objective(self, probs: np.ndarray) -> float:
        """Compute ambiguity objective."""
        total_ambiguity = 0
        for idx in range(self.n_pairs):
            i, j = self._index_to_pair(idx)
            if self.battle_counts[i, j] > 0:
                avg_preference = (
                    self.battle_preferences[i, j] / self.battle_counts[i, j]
                )
                ambiguity_score = 1.0 - abs(avg_preference)
                total_ambiguity -= probs[idx] * ambiguity_score
        return total_ambiguity

    def objective_function(self, theta: np.ndarray) -> float:
        """Combined objective function for optimization."""
        # Convert theta to probabilities
        probs = np.exp(theta) / np.sum(np.exp(theta))

        # Compute individual objectives
        latency_obj = self.compute_latency_objective(probs)
        rarity_obj = self.compute_rarity_objective(probs)
        ambiguity_obj = self.compute_ambiguity_objective(probs)

        # Combine objectives with weights
        total_obj = (
            self.lambda_latency * latency_obj
            + self.lambda_rarity * rarity_obj
            + self.lambda_ambiguity * ambiguity_obj
        )

        return total_obj

    def fit(self, max_iter: int = 1000):
        """Optimize the routing parameters."""
        # Create a wrapper function that updates the progress bar
        pbar = tqdm(total=max_iter, desc="Optimizing routing parameters")
        iter_count = [0]  # Use list to allow modification in nested function

        def objective_with_progress(x):
            iter_count[0] += 1
            pbar.update(1)
            print(self._softmax_function(self.theta))
            return self.objective_function(x)

        try:
            result = minimize(
                objective_with_progress,
                self.theta,
                method="L-BFGS-B",
                options={"maxiter": max_iter},
            )
            self.theta = result.x
            return result
        finally:
            pbar.close()

    def get_routing_probabilities(self, temp=1.0) -> Dict[Tuple[str, str], float]:
        """Get the optimized routing probabilities for each model pair."""
        probs = self._softmax_function(theta=self.theta, temp=temp)
        routing_probs = {}

        for idx in range(self.n_pairs):
            i, j = self._index_to_pair(idx)
            model_i, model_j = self.models[i], self.models[j]
            routing_probs[(model_i, model_j)] = probs[idx]

        return routing_probs

    def sample_model_pair(self) -> Tuple[str, str]:
        """Sample a model pair according to the optimized distribution."""
        probs = self._softmax_function(theta=self.theta)
        idx = np.random.choice(self.n_pairs, p=probs)
        i, j = self._index_to_pair(idx)
        return self.models[i], self.models[j]

    def visualize_probability_matrix(self, temp=1.0):
        """Create and display a probability matrix for all model pairs."""
        import matplotlib.pyplot as plt
        import seaborn as sns

        # Initialize probability matrix
        prob_matrix = np.zeros((self.n_models, self.n_models))

        # Get probabilities
        probs = self._softmax_function(theta=self.theta, temp=temp)

        # Fill the matrix
        for idx in range(self.n_pairs):
            i, j = self._index_to_pair(idx)
            prob = probs[idx]
            # Fill both sides of the matrix
            prob_matrix[i, j] = prob
            prob_matrix[j, i] = prob

        # Create figure
        plt.figure(figsize=(15, 12))

        # Create heatmap
        sns.heatmap(
            prob_matrix,
            xticklabels=self.models,
            yticklabels=self.models,
            annot=True,  # Show probabilities in cells
            fmt=".3f",  # Format probabilities to 3 decimal places
            cmap="YlOrRd",
        )

        plt.title("Model Pairing Probabilities")
        plt.xticks(rotation=45, ha="right")
        plt.yticks(rotation=0)
        plt.tight_layout()

        # Return the matrix for further analysis if needed
        return prob_matrix

    def print_probability_matrix(self, temp=1.0, title="", file_path="probability_matrix.txt"):
        """Append the probability matrix in a formatted table to a file."""
        with open(file_path, "a") as f:
            if title:
                f.write(title + "\n")
            probs = self._softmax_function(theta=self.theta, temp=temp)
            prob_matrix = np.zeros((self.n_models, self.n_models))

            # Fill the matrix
            for idx in range(self.n_pairs):
                i, j = self._index_to_pair(idx)
                prob = probs[idx]
                prob_matrix[i, j] = prob
                prob_matrix[j, i] = prob

            # Write header
            f.write("\nProbability Matrix:\n")
            f.write("-" * 120 + "\n")
            f.write(f"{'Model':30}")
            for model in self.models:
                f.write(f"{model:>10}")
            f.write("\n" + "-" * 120 + "\n")

            # Write rows
            for i, model1 in enumerate(self.models):
                f.write(f"{model1:30}")
                for j, model2 in enumerate(self.models):
                    if i == j:
                        f.write(f"{'---':>10}")
                    else:
                        f.write(f"{prob_matrix[i,j]:10.3f}")
                f.write("\n")
            f.write("-" * 120 + "\n")

        return prob_matrix

    def calculate_expected_latency(self, temp: float = 1.0) -> float:
        """
        Calculate the expected latency across all model pairs given the current routing probabilities.

        Args:
            temp (float): Temperature parameter for softmax probability calculation

        Returns:
            float: Expected latency in seconds
        """
        if not self.latency_params:
            raise ValueError(
                "Latency parameters not fitted. Call fit_latency_parameters first."
            )

        # Get current routing probabilities
        probs = self._softmax_function(theta=self.theta, temp=temp)
        total_expected_latency = sum(
            [probs[idx] * self.latencies[idx] for idx in range(self.n_pairs)]
        )

        return total_expected_latency

    def print_expected_latencies(
        self, temperatures: List[float] = [1.0, 2.0, 5.0, 10.0]
    ):
        """
        Print expected latencies for different temperature values.

        Args:
            temperatures (List[float]): List of temperature values to evaluate
        """
        print("\nExpected Latencies:")
        print("-" * 50)
        print(f"{'Temperature':>12} | {'Expected Latency (s)':>20}")
        print("-" * 50)

        for temp in temperatures:
            expected_latency = self.calculate_expected_latency(temp)
            print(f"{temp:12.1f} | {expected_latency:20.3f}")
        print("-" * 50)


# Example usage
def main():
    models = [
        "gpt-4o-mini-2024-07-18",
        "codestral-2405",
        "llama-3.1-70b-instruct",
        "llama-3.1-405b-instruct",
        "gemini-1.5-flash-002",
        "gemini-1.5-pro-002",
        "claude-3-5-sonnet-20240620",
        "claude-3-5-sonnet-20241022",
        "qwen-2.5-coder-32b-instruct",
        "gpt-4o-2024-08-06",
    ]
    # Initialize router with the models list
    lambda_latency = 0.1
    lambda_rarity = 1
    lambda_ambiguity = 1
    router = ModelRouter(
        models,
        lambda_latency=lambda_latency,
        lambda_rarity=lambda_rarity,
        lambda_ambiguity=lambda_ambiguity,
    )

    # Load the dataframes from csv
    global_completions_df = pd.read_csv("completions_data.csv")
    global_outcomes_df = pd.read_csv("outcomes_data.csv")

    # Fit latency parameters
    router.fit_latency_parameters(global_completions_df)
    router.compute_latency()
    # Compute battle statistics
    router.compute_battle_statistics(global_outcomes_df)

    # Define ranges for lambda parameter sweeps
    lambda_latency_values = np.arange(0, 1, 0.1)
    lambda_rarity_values = np.arange(0, 1, 0.1)
    lambda_ambiguity_values = np.arange(0, 1, 0.1)

    # Iterate over all combinations of lambda values
    for lambda_latency in lambda_latency_values:
        for lambda_rarity in lambda_rarity_values:
            for lambda_ambiguity in lambda_ambiguity_values:
                # Update router's lambda values
                router.lambda_latency = lambda_latency
                router.lambda_rarity = lambda_rarity
                router.lambda_ambiguity = lambda_ambiguity

                filename = "routing_params/routing_parameters_{}_{}_{}.json".format(
                    lambda_latency, lambda_rarity, lambda_ambiguity
                )

                # Load the routing_parameters if it exists
                try:
                    with open(filename, "r") as f:
                        routing_parameters = json.load(f)
                        router.theta = np.array(routing_parameters["theta"])
                except FileNotFoundError:
                    # Optimize routing parameters
                    result = router.fit()
                    print(f"Optimization completed for lambda values ({lambda_latency}, {lambda_rarity}, {lambda_ambiguity}): {result.success}")
                # Save the result
                with open(filename, "w") as f:
                    json.dump({"theta": router.theta.tolist()}, f)

                # Explore routing probabilities with different temperatures
                temperatures = [1.0]
                for temp in temperatures:
                    routing_probs = router.get_routing_probabilities(temp=temp)
                    sorted_pairs = sorted(
                        routing_probs.items(), key=lambda x: x[1], reverse=True
                    )

                    # out_f.write(
                    #     f"Top 10 model pairs by routing probability (temperature={temp:.1f}):"
                    # )
                    # for (model1, model2), prob in sorted_pairs[:10]:
                    #     out_f.write(f"{model1} vs {model2}: {prob:.4f}")

                    # Print text version and append output to file
                    router.print_probability_matrix(temp=temp)

                router.print_expected_latencies(temperatures)


if __name__ == "__main__":
    main()