import pandas as pd
import os
import random
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score
from torch.nn import functional as F
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import seaborn as sns
from colpali_engine.interpretability import (
    get_similarity_maps_from_embeddings,
    plot_all_similarity_maps,
)


# Ruta al conjunto de datos Flickr8k extraído
FLICKR8K_IMAGES_PATH = "flickr8k/Images"
FLICKR8K_CAPTIONS_PATH = "flickr8k/captions.txt"

# Función para cargar pares de imagen-texto de Flickr8k


def load_flickr8k_data(images_path, captions_path, fraction=0.1):
    # Leer archivo de subtítulos
    with open(captions_path, "r") as f:
        captions_data = f.readlines()[1:]  # Omitir encabezado

    # Analizar subtítulos
    image_text_pairs = {}
    for line in captions_data:
        image_name, caption = line.strip().split(",", 1)
        if image_name not in image_text_pairs:
            image_text_pairs[image_name] = []
        image_text_pairs[image_name].append(caption)

    # Cargar solo una fracción del conjunto de datos
    selected_images = random.sample(
        list(image_text_pairs.keys()), int(len(image_text_pairs) * fraction)
    )
    image_text_pairs = {k: image_text_pairs[k] for k in selected_images}

    # Crear pares de imágenes y subtítulos
    pairs = []
    for image_name, captions in image_text_pairs.items():
        image_path = os.path.join(images_path, image_name)
        if os.path.exists(image_path):
            pairs.append((Image.open(image_path), random.choice(captions)))
    return pairs


# Función para crear pares no relacionados


def create_unrelated_pairs(image_text_pairs):
    """Crea pares no relacionados de imágenes y textos al mezclar aleatoriamente los textos.

Args:
    image_text_pairs (list): Una lista de tuplas que contiene imágenes y sus textos correspondientes.

Returns:
    list: Una lista de tuplas que contiene imágenes y textos no relacionados."""
    images, texts = zip(*image_text_pairs)
    unrelated_texts = random.sample(texts, len(texts))
    return list(zip(images, unrelated_texts))


def create_visual_pairs(image_text_pairs):
    """Crea pares de imágenes originales y aumentadas a partir de pares de imagen-texto.

Esta función toma una lista de pares de imagen-texto y crea nuevos pares que consisten en las imágenes originales y sus versiones aumentadas. La augmentación utilizada en esta implementación es un volteo horizontal.

Argumentos:
    image_text_pairs (list): Una lista de tuplas que contienen pares (imagen, texto),
        donde las imágenes son objetos PIL Image y los textos son cadenas de caracteres.

Devuelve:
    list: Una lista de tuplas que contienen pares (imagen_original, imagen_aumentada),
        donde ambos elementos son objetos PIL Image."""
    from torchvision.transforms import ToTensor

    images, _ = zip(*image_text_pairs)
    # Ejemplo de aumento: volteo horizontal
    augmented_images = [ToTensor()(image).flip(-1) for image in images]
    return list(zip(images, augmented_images))


def get_embeddings(images, texts, model_id="google/siglip-base-patch16-224"):
    """Dadas listas de imágenes y textos, devuelve embeddings normalizados para ambos."""
    # Asegúrate de que texts sea una lista de cadenas
    if not all(isinstance(t, str) for t in texts):
        raise ValueError("All text inputs must be strings.")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModel.from_pretrained(
        model_id, ignore_mismatched_sizes=True).to(device)
    processor = AutoProcessor.from_pretrained(model_id)

    # Preprocesar imágenes y textos
    image_inputs = processor(images=images, return_tensors="pt").to(device)
    text_inputs = processor(text=texts, return_tensors="pt", padding="max_length").to(
        device
    )

    with torch.no_grad():
        image_embeds = model.get_image_features(**image_inputs)
        text_embeds = model.get_text_features(**text_inputs)

    # Normalizar incrustaciones
    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)

    return image_embeds, text_embeds


def cosine_similarity_analysis(embeddings1, embeddings2, title):
    """Calcula la similitud coseno para pares coincidentes y no relacionados y compara distribuciones."""
    similarities = cosine_similarity(
        embeddings1.cpu().numpy(), embeddings2.cpu().numpy()
    )

    # Pares coincidentes: Diagonal de la matriz de similitud
    matching_similarities = np.diag(similarities)

    # Pares no relacionados: Similitudes fuera de la diagonal
    unrelated_similarities = similarities[~np.eye(
        similarities.shape[0], dtype=bool)]

    print(f"### {title} ###")
    print(f"Mean Matching Similarity: {np.mean(matching_similarities):.4f}")
    print(f"Mean Unrelated Similarity: {np.mean(unrelated_similarities):.4f}")
    print()

    # Graficar distribuciones
    plt.figure(figsize=(10, 6))
    sns.histplot(
        matching_similarities, kde=True, label="Matching Pairs", color="blue", bins=30
    )
    sns.histplot(
        unrelated_similarities, kde=True, label="Unrelated Pairs", color="red", bins=30
    )
    plt.title(f"{title}: Cosine Similarity Distributions")
    plt.xlabel("Cosine Similarity")
    plt.ylabel("Frequency")
    plt.legend()
    plt.show()


# b. Recuperación de Vecinos Más Cercanos


def retrieval_metrics(query_embeds, target_embeds, ground_truth_indices, k=5):
    """Calcula Precision@k y Recall@k para la recuperación de vecinos más cercanos.

Esta función evalúa la efectividad de la recuperación calculando Precision@k y Recall@k.
Precision@k mide la precisión de los elementos recuperados en el top-k, mientras que Recall@k mide la capacidad
de encontrar el elemento relevante dentro de los elementos recuperados en el top-k. Se asume que hay solo una coincidencia verdadera por consulta.

Args:
    query_embeds (torch.Tensor): Embeddings de los datos de consulta.
    target_embeds (torch.Tensor): Embeddings de los datos objetivo (base de datos).
    ground_truth_indices (list): Lista de índices en los datos objetivo que representan las coincidencias verdaderas para cada consulta.
    k (int): El número de mejores resultados a considerar.

Returns:
    tuple: Una tupla que contiene la media de Precision@k y la media de Recall@k."""
    similarities = cosine_similarity(
        query_embeds.cpu().numpy(), target_embeds.cpu().numpy()
    )
    sorted_indices = np.argsort(-similarities, axis=1)[:, :k]  # Índices top-k

    # Calcular métricas
    precisions = []
    recalls = []
    for i, true_idx in enumerate(ground_truth_indices):
        retrieved_indices = sorted_indices[i]
        true_positives = int(true_idx in retrieved_indices)
        precisions.append(true_positives / k)
        recalls.append(true_positives / 1)  # Solo una coincidencia verdadera por consulta

    mean_precision = np.mean(precisions)
    mean_recall = np.mean(recalls)

    return mean_precision, mean_recall


def plot_query_token_importance(
    pil_image, similarity_maps, query_tokens, alpha: float = 0.5
) -> None:
    """Genera un mapa de calor separado para cada token de consulta en los similarity_maps.

Args:
    pil_image (PIL.Image.Image): La imagen original (por ejemplo, cargada a través de Image.open(...)).
    similarity_maps (torch.Tensor):
        Forma = (num_query_tokens, n_patches_x, n_patches_y).
    query_tokens (List[str]): Una lista de cadenas para cada token en la consulta.
    alpha (float): Transparencia para las superposiciones del mapa de calor (0=transparente, 1=opaco)."""
    # Convertir PIL a numpy
    image_np = np.array(pil_image)
    H, W = image_np.shape[:2]

    num_tokens = similarity_maps.size(0)
    assert num_tokens == len(query_tokens), (
        f"The number of query tokens in similarity_maps ({num_tokens}) "
        f"doesn't match the length of query_tokens list ({len(query_tokens)})."
    )

    fig, axs = plt.subplots(1, num_tokens, figsize=(5 * num_tokens, 5))
    if num_tokens == 1:
        # Si solo hay un token, axs no será un iterable
        axs = [axs]

    for idx in range(num_tokens):
        # Cada similarity_map para un único token de consulta: forma = (n_patches_x, n_patches_y)
        single_map = similarity_maps[idx]  # (n_patches_x, n_patches_y)

        # Ampliar a tamaño completo de la imagen
        single_map_4d = single_map.unsqueeze(0).unsqueeze(
            0
        )  # (1,1,n_patches_x, n_patches_y)
        upsampled = F.interpolate(
            single_map_4d, size=(H, W), mode="bilinear", align_corners=False
        )

        # .to(torch.float32) corrige si tu mapa es bfloat16
        heatmap = upsampled.squeeze().to(torch.float32).cpu().numpy()  # (H, W)

        # Opcionalmente normalizar el mapa de calor (descomentar si se desea)
        # heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)

        # Graficar
        axs[idx].imshow(image_np, cmap=None if image_np.ndim == 3 else "gray")
        axs[idx].imshow(heatmap, cmap="jet", alpha=alpha)
        axs[idx].set_title(f"Query: {query_tokens[idx]}")
        axs[idx].axis("off")

    plt.tight_layout()
    plt.show()


def get_maps_and_embeds(
    batch_images, batch_queries, model, processor, image, use_qwen=False
):
    """Calcula mapas de similitud y embeddings a partir de un lote de imágenes y consultas utilizando el modelo y procesador especificados.

Args:
    batch_images (dict): Un diccionario de entradas de imágenes en lotes procesadas por el procesador.
    batch_queries (dict): Un diccionario de entradas de consultas en lotes procesadas por el procesador.
    model (nn.Module): El modelo utilizado para calcular los embeddings.
    processor (Processor): El procesador responsable de la preprocesamiento de imágenes y texto.

Returns:
    tuple: Una tupla que contiene:
        - original_maps (torch.Tensor): Mapas de similitud entre imágenes y consultas
            con forma (num_queries, n_patches_x, n_patches_y).
        - original_image_embeddings (torch.Tensor): Embeddings de las imágenes de entrada.
        - original_query_embeddings (torch.Tensor): Embeddings de las consultas de entrada."""
    with torch.no_grad():
        original_image_embeddings = model.forward(**batch_images)
        original_query_embeddings = model.forward(**batch_queries)
    if use_qwen:
        n_patches = processor.get_n_patches(
            image_size=image.size,
            patch_size=model.patch_size,
            spatial_merge_size=model.spatial_merge_size,
        )
    else:
        n_patches = processor.get_n_patches(
            image_size=image.size, patch_size=model.patch_size
        )
    image_mask = processor.get_image_mask(batch_images)

    # Calcular mapas de similitud originales
    original_batched_maps = get_similarity_maps_from_embeddings(
        image_embeddings=original_image_embeddings,
        query_embeddings=original_query_embeddings,
        n_patches=n_patches,
        image_mask=image_mask,
    )
    # (longitud_consulta, n_parches_x, n_parches_y)
    original_maps = original_batched_maps[0].permute(0, 2, 1).contiguous()
    return original_maps, original_image_embeddings, original_query_embeddings


def visualize_token_map(
    image,
    original_maps,
    token_list,
    token_index=2,
    cmap="Greens",
    figsize=(15, 2),
    show_text=True,
):
    """
    Visualize a token's attention map in three ways: the original image, the raw attention map with numerical values,
    and an overlay of the attention map on the original image.
    Args:
        image (PIL.Image): The input image to visualize.
        original_maps (torch.Tensor or np.ndarray): Attention maps with shape (num_tokens, height, width).
        token_list (list[str]): List of token strings corresponding to each attention map.
        token_index (int, optional): Index of the token/map to visualize. Defaults to 2.
        cmap (str, optional): Matplotlib colormap name for visualizing the attention maps. Defaults to "Greens".

    The function creates a figure with three subplots:
    1. The original input image
    2. The raw attention map with numerical values annotated
    3. The attention map overlaid on the original image with a colorbar

    Returns:
        None. Displays the visualization using matplotlib.
    """
    # Convertir la imagen a un array de NumPy
    image_np = np.array(image)

    # Selecciona el mapa correspondiente al token
    visual_map = original_maps[token_index]

    # Convertir visual_map a un array de NumPy si es un tensor
    if isinstance(visual_map, torch.Tensor):
        visual_map = visual_map.cpu().to(dtype=torch.float32).numpy()
    elif not isinstance(visual_map, np.ndarray):
        visual_map = np.array(visual_map)

    # Convertir el mapa a una imagen PIL
    visual_map_pil = Image.fromarray(visual_map)

    # Redimensionar usando NEAREST para mantener "pixeles grandes
    visual_map_pil = visual_map_pil.resize(
        (image_np.shape[1], image_np.shape[0]),  # (ancho, alto)
        resample=Image.NEAREST,
    )

    # Convertir de nuevo a NumPy
    resized_map = np.array(visual_map_pil)

    # Crear una figura con subgráficas
    fig, axes = plt.subplots(1, 3, figsize=(15, 2))

    # Mostrar la imagen sin procesar
    axes[0].imshow(image_np)
    axes[0].set_title("Raw Image")
    axes[0].axis("off")
    # Mostrar el mapa sin procesar con anotaciones
    im = axes[1].imshow(visual_map, cmap=cmap)
    axes[1].set_title("Raw Map")
    axes[1].axis("off")

    if show_text:
        # Anotar el mapa de calor
        for i in range(visual_map.shape[0]):
            for j in range(visual_map.shape[1]):
                text = axes[1].text(
                    j,
                    i,
                    f"{visual_map[i, j]:.2f}",
                    ha="center",
                    va="center",
                    color="w" if visual_map[i, j] > visual_map.max() / 2 else "black",
                )

    # Mostrar el gráfico superpuesto
    axes[2].imshow(image_np, alpha=1)
    axes[2].imshow(resized_map, cmap=cmap, alpha=0.6)
    axes[2].set_title("Overlay: Image + Map")
    axes[2].axis("off")
    # Añadir una barra de color para la superposición con valores coincidentes con el mapa original
    cbar = fig.colorbar(
        plt.cm.ScalarMappable(
            cmap=cmap, norm=plt.Normalize(vmin=visual_map.min(), vmax=visual_map.max())
        ),
        ax=axes[2],
        shrink=0.8,
        orientation="vertical",
    )
    cbar.set_label("Map Intensity")
    # Añadir un título con el nombre del token
    plt.suptitle(f"Token: {token_list[token_index]}")

    # Ajustar diseño y mostrar
    plt.tight_layout()
    plt.show()


def create_single_patch_image(
    n_patches_x,
    n_patches_y,
    patch_size,
    main_color,
    special_color,
    special_patch,
    special_patch_width=2,
):
    """
    Creates an image composed of colored patches, with one special patch highlighted.

    The image is divided into a grid of n_patches_x by n_patches_y patches, each of size
    patch_size x patch_size pixels. All patches are filled with the main_color, except
    for the special_patch, which is filled with special_color.  The special patch can
    also have a width of more than one patch.
    Args:
        n_patches_x (int): Number of patches horizontally.
        n_patches_y (int): Number of patches vertically.
        patch_size (int): The size (in pixels) of each square patch.
        main_color (list): The [R, G, B] color for most patches.
        special_color (list): The [R, G, B] color for the special patch.
        special_patch (tuple): The (row, col) position of the top-left corner of the special patch (0-indexed).
        special_patch_width (int, optional): The width of the special patch in number of patches. Defaults to 2.

    Returns:
        PIL Image: The generated image.
    """

    # Crea un array 3D de NumPy para la imagen
    img_height = n_patches_y * patch_size
    img_width = n_patches_x * patch_size
    image_data = np.zeros((img_height, img_width, 3), dtype=np.uint8)

    # Rellenar toda la imagen con el color principal
    image_data[:, :] = main_color

    # Asignar el color especial al parche especial
    special_row, special_col = special_patch
    image_data[
        special_row * patch_size: (special_row + special_patch_width) * patch_size,
        special_col * patch_size: (special_col + special_patch_width) * patch_size,
    ] = special_color

    return Image.fromarray(image_data)


def extract_patch_mask(image, patch_size, special_color=[0, 0, 0]):
    """Extraer una máscara binaria que indique la ubicación del parche especial.

Argumentos:
    image (PIL.Image.Image): La imagen de entrada.
    patch_size (int): El tamaño de cada parche cuadrado en píxeles.
    special_color (list[int]): El color RGB del parche especial.

Devuelve:
    np.ndarray: Una máscara binaria de forma (n_patches_y, n_patches_x) que indica
                la ubicación del parche especial (1 para parche especial, 0 de lo contrario)."""
    # Convertir la imagen a un array de NumPy
    image_np = np.array(image)

    # Obtener dimensiones de la imagen
    img_height, img_width, _ = image_np.shape

    # Calcular el número de parches
    n_patches_y = img_height // patch_size
    n_patches_x = img_width // patch_size

    # Inicializar la máscara de parches
    patch_mask = np.zeros((n_patches_y, n_patches_x), dtype=np.int32)

    # Iterar sobre todos los parches para localizar el parche especial
    for row in range(n_patches_y):
        for col in range(n_patches_x):
            # Extraer el parche
            patch = image_np[
                row * patch_size: (row + 1) * patch_size,
                col * patch_size: (col + 1) * patch_size,
            ]

            # Verificar si el parche coincide con el color especial
            if np.allclose(patch.mean(axis=(0, 1)), special_color, atol=1e-6):
                patch_mask[row, col] = 1  # Marcar este parche como especial

    return patch_mask


def evaluate_map_quality(similarity_map, patch_mask):
    """Evaluar la calidad de un mapa de similitud con respecto a una máscara binaria de parche.

Args:
    similarity_map (torch.Tensor): El mapa de similitud (altura, ancho).
    patch_mask (np.ndarray): La máscara binaria para el parche (1 para parche negro, 0 en otros lugares).

Returns:
    dict: Métricas que incluyen correlación, precisión máxima y puntuación de superposición."""
    # Asegúrate de que similarity_map esté en float32 y en la CPU antes de usar operaciones de numpy
    if isinstance(similarity_map, np.ndarray):
        similarity_map_cpu = similarity_map.astype(np.float32)
    else:
        similarity_map_cpu = similarity_map.to(dtype=torch.float32).cpu().numpy()

    # Aplanar el mapa y la máscara para facilitar el cálculo
    sim_map_flat = similarity_map_cpu.flatten()
    patch_mask_flat = patch_mask.flatten()

    # Asegúrate de que las formas sean compatibles
    if sim_map_flat.shape != patch_mask_flat.shape:
        raise ValueError(
            f"Shape mismatch: similarity_map has {sim_map_flat.shape} elements, "
            f"but patch_mask has {patch_mask_flat.shape} elements."
        )

    # (A) Correlación
    correlation = np.corrcoef(
        sim_map_flat, patch_mask_flat.astype(np.float32))[0, 1]

    # (B) Ubicación del Pico de Señal
    max_location = np.unravel_index(
        np.argmax(similarity_map), similarity_map.shape)
    expected_location = np.unravel_index(
        np.argmax(patch_mask), patch_mask.shape)
    peak_accuracy = 1 if max_location == expected_location else 0

    # (C) Superposición de Mapa Normalizado
    black_patch_score = similarity_map[patch_mask == 1].mean()
    background_score = similarity_map[patch_mask == 0].mean()
    overlap_score = black_patch_score / (
        background_score + 1e-8
    )  # Evitar la división por cero

    # Devuelve todas las métricas
    return {
        "correlation": correlation,
        "peak_accuracy": peak_accuracy,
        "overlap_score": overlap_score,
    }


def evaluate_image_maps(similarity_map, real_image):
    """Evalúa la calidad de los mapas de similitud comparándolos con una imagen real.

Args:
    similarity_map (torch.Tensor): El mapa de similitud a evaluar.
    real_image (PIL.Image.Image): La imagen real correspondiente.

Returns:
    dict: Un diccionario que contiene las métricas calculadas: precisión, puntuación y rango."""
    # Convertir la imagen real a un arreglo binario (1 - escala de grises normalizada)
    image_array = 1 - np.array(real_image.convert("L"),
                               dtype=np.float32) / 255.0

    # Asegúrate de que similarity_map sea float32 y esté en la CPU antes de usar operaciones de numpy
    if isinstance(similarity_map, np.ndarray):
        similarity_map_cpu = similarity_map.astype(np.float32)
    else:
        similarity_map_cpu = similarity_map.to(dtype=torch.float32).cpu().numpy()

    # Crear una máscara para los valores máximos en el mapa de similitud
    acc_visual_map = np.where(
        similarity_map_cpu == similarity_map_cpu.max(), similarity_map_cpu, 0
    )

    # Verificar si es necesario escalar
    if image_array.shape != similarity_map_cpu.shape:
        scale_factor = image_array.shape[0] // similarity_map_cpu.shape[0]
        scaled_visual_map = np.kron(
            np.abs(similarity_map_cpu), np.ones((scale_factor, scale_factor))
        )
        rank_map = np.kron(
            np.abs(similarity_map_cpu), np.ones((scale_factor, scale_factor))
        )
        acc_visual_map = np.kron(
            np.abs(acc_visual_map), np.ones((scale_factor, scale_factor))
        )
    else:
        scaled_visual_map = similarity_map_cpu
        rank_map = similarity_map_cpu  # Agregar esto para evitar la falta de variable

    # Calcular precisión y puntuación
    accuracy = np.any(image_array * acc_visual_map)
    score = np.sum(image_array * scaled_visual_map) / (
        np.sum(image_array) + 1e-8
    )  # Evitar la división por cero

    # Calcular rango
    bin_image = (image_array != 0).astype(int)
    rank_value = np.sum(bin_image * rank_map) / np.sum(
        bin_image
    )  # Evitar la división por cero
    sorted_values = sorted(np.abs(similarity_map_cpu.ravel()))[::-1]
    rank = np.where(np.isclose(sorted_values, rank_value))[0][0]

    return {
        "accuracy": accuracy,
        "score": score,
        "rank": rank,
    }


def create_single_patch_image_with_text(
    n_patches_x,
    n_patches_y,
    patch_size,
    main_color,
    special_color,
    special_patch,
    text="Hello",
    text_color=(255, 255, 255),
    special_patch_width=2,
    font_size=16,
    # Agregado parámetro font_path con valor por defecto
    font_path="./fonts/Roboto-Regular.ttf",
):
    """Crea una imagen compuesta de parches de colores, pero coloca una sola palabra (o texto) dentro del área del parche 'especial'."""
    # Crear un array 3D de NumPy para la imagen
    img_height = n_patches_y * patch_size
    img_width = n_patches_x * patch_size
    image_data = np.zeros((img_height, img_width, 3), dtype=np.uint8)

    # Rellenar toda la imagen con el color principal
    image_data[:, :] = main_color

    # Asignar el color especial al área del parche especial
    special_row, special_col = special_patch
    image_data[
        special_row * patch_size: (special_row + special_patch_width) * patch_size,
        special_col * patch_size: (special_col + special_patch_width) * patch_size,
    ] = special_color

    # Convertir a una imagen de Pillow para que podamos dibujar en ella
    img = Image.fromarray(image_data)
    draw = ImageDraw.Draw(img)

    # Cargar fuente con el tamaño especificado
    try:
        font = ImageFont.truetype(font_path, font_size)
    except IOError:
        print(f"Error loading font from {font_path}. Using default font.")
        font = ImageFont.load_default()

    # Calcular el centro del parche especial en coordenadas de píxeles
    patch_center_x = special_col * patch_size + (special_patch_width * patch_size) // 2
    patch_center_y = special_row * patch_size + (special_patch_width * patch_size) // 2

    # Calcular el cuadro delimitador del texto para centrar el texto
    text_bbox = draw.textbbox((0, 0), text, font=font)
    text_width = text_bbox[2] - text_bbox[0]
    text_height = text_bbox[3] - text_bbox[1]

    text_x = patch_center_x - text_width // 2
    text_y = patch_center_y - text_height // 2

    # Colocar el texto en el centro del parche especial
    draw.text((text_x, text_y), text, fill=text_color, font=font)

    return img


def visualize_results_grid(results_df):
    columns = [results_df.iloc[:, i] for i in range(len(results_df.columns))]
    columns = [
        (
            pd.to_numeric(col, errors="coerce")
            if not pd.api.types.is_numeric_dtype(col)
            else col
        )
        for col in columns
    ]

    # Deducir la forma de la cuadrícula a partir del número de filas de resultados
    grid_size = int(np.sqrt(len(results_df)))
    # Reformar columnas en matrices
    matrices = [col.to_numpy().reshape(grid_size, grid_size)
                for col in columns]

    # Configuración de visualización
    fig, axes = plt.subplots(1, len(results_df.columns), figsize=(12, 2))
    titles = [
        (
            f"{results_df.columns[i]} (Categorical/Binary)"
            if i == 0
            else f"{results_df.columns[i]} (Continuous)"
        )
        for i in range(len(results_df.columns))
    ]
    # Añadido mapa de colores para el cuarto gráfico
    cmaps = ["coolwarm"] * len(results_df.columns)
    # Graficar cada matriz
    for i, (matrix, ax, title, cmap) in enumerate(zip(matrices, axes, titles, cmaps)):
        im = ax.imshow(matrix, cmap=cmap, interpolation="none")
        ax.set_title(title)
        ax.set_xticks(range(grid_size))
        ax.set_yticks(range(grid_size))
        fig.colorbar(im, ax=ax)

    # Mostrar el gráfico
    plt.tight_layout()
    plt.show()


def run_expe_word_square(
    word_to_write,
    token,
    n_patches_x,
    n_patches_y,
    patch_size,
    model,
    processor,
    device,
    use_qwen,
    main_color=[255, 255, 255],
    special_color=(0, 0, 0),
):

    all_images_text = [
        create_single_patch_image_with_text(
            n_patches_x=n_patches_x,
            n_patches_y=n_patches_y,
            patch_size=patch_size,
            main_color=main_color,
            special_color=main_color,
            special_patch=(row, col),
            text=word_to_write,
            text_color=(0, 0, 0),  # color_texto,
            font_size=9,
        )
        for row in range(0, n_patches_y, 2)
        for col in range(0, n_patches_x, 2)
    ]

    all_maps = []
    for image in all_images_text:
        batch_images = processor.process_images([image]).to(device)
        batch_queries = processor.process_queries([token]).to(device)
        original_maps, original_image_embeddings, original_query_embeddings = (
            get_maps_and_embeds(
                batch_images, batch_queries, model, processor, image, use_qwen=use_qwen
            )
        )
        original_maps = original_maps.to(dtype=torch.float32).cpu().numpy()
        all_maps.append(original_maps)

    input_ids = batch_queries["input_ids"][0]  # forma: (num_subtokens,)
    token_list = [processor.tokenizer.decode(
        [token_id]) for token_id in input_ids]
    # imprimir(token_list)
    indexes = [i for i, x in enumerate(
        token_list) if "<" not in x and ">" not in x][2:]
    # imprimir(indexes)
    # print(np.array(token_list)[[indexes]])

    results_df = pd.DataFrame(columns=["accuracy", "score", "rank"])
    for i, (this_map, image) in enumerate(zip(all_maps, all_images_text)):
        visual_map = this_map[indexes[0]]
        metrics = evaluate_image_maps(visual_map, image)
        results_df.loc[i] = metrics.values()
    return results_df